<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title type="text">Majid alDosari blog feed</title>
  <id>urn:uuid:4b540731-1183-3f99-9b05-49948e40f741</id>
  <updated>2019-02-11T00:00:00Z</updated>
  <link href="https://majidaldo.github.io/blog/" />
  <link href="https://majidaldo.github.io/" rel="self" />
  <author>
    <name></name>
  </author>
  <generator uri="https://github.com/ajdavis/lektor-atom" version="0.3">Lektor Atom Plugin</generator>
  <entry xml:base="https://majidaldo.github.io/blog/RAPIDS-op/">
    <title type="text">Hello Website2</title>
    <id>urn:uuid:4323bacc-0023-3966-91c8-7d74a31285a4</id>
    <updated>2019-02-11T00:00:00Z</updated>
    <link href="https://majidaldo.github.io/blog/RAPIDS-op/" />
    <author>
      <name></name>
    </author>
    <content type="html">&lt;p&gt;This is an example blog post.  Not much here but that's not the point :)&lt;/p&gt;
</content>
  </entry>
  <entry xml:base="https://majidaldo.github.io/blog/plot-entropy/">
    <title type="text">Plot entropy can be used to automatically select scatter plot transparency</title>
    <id>urn:uuid:415a0784-62e4-3615-afe4-a89ded6f194b</id>
    <updated>2018-12-02T00:00:00Z</updated>
    <link href="https://majidaldo.github.io/blog/plot-entropy/" />
    <author>
      <name></name>
    </author>
    <content type="html">&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: As a data scientist, you make scatter plots to assess visually a distribution of points. However, many times the points are too dense which can give you a wrong impression about the point distribution. So, you might adjust the transparency setting (usually called alpha) a few times. Now, let's say you change the dataset. Again, repeat adjusting the transparency.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Optimize the plot image entropy because it quantifies the 'variety' of color in it.&lt;/p&gt;
&lt;p&gt;Why?&lt;/p&gt;
&lt;p&gt;When you adjust the transparency, you are eyeballing a measure of image color 'variety'. I went for an information-theortic measure of 'variety'/'dispersion' as opposed to a statistical one (like standard deviation). What I like about information theory is that I can be less mindful of specific statistical distributions and models.&lt;/p&gt;
&lt;p&gt;The magic line to calculate color image entropy with scipy/numpy is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;entropy(histogramdd(img, bins=[arange(256)]*3)[0].flatten(), base=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where img is shape (width*height, 3) with 8-bit resolution for each color.&lt;/p&gt;
&lt;p&gt;The below figures show a monochromatic and color version of a data set scatter plot with alpha (a) and entropy (s) annotated at the top right. The alpha was varied which results in different entropy values. The red annotation signifies the highest entropy.&lt;/p&gt;
&lt;p&gt;I think the utility and quantification is easier to see in the monochromatic version perhaps due to it being less 'busy'. But the numbers don't lie!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/blog/plot-entropy/color_entropy.png&quot; alt=&quot;color&quot;&gt; |  &lt;img src=&quot;/blog/plot-entropy/mono_entropy.png&quot; alt=&quot;mono&quot;&gt;&lt;/p&gt;
&lt;p&gt;I'd like to package this somehow if there is interest.&lt;/p&gt;
</content>
  </entry>
</feed>
